p8105_hw5_lmp2234
================
Lisa Pardee
2024-11-13

## Problem 1.

``` r
birthday_df <- function(n) {
  birthday <- sample(1:365, n, replace = TRUE)
  return(anyDuplicated(birthday)>0)
}

set.seed(42)
group_sizes <- 2:50
probabilities <- numeric(length(group_sizes))

for (i in group_sizes){
  simulations <- replicate(10000, birthday_df(i))
  probabilities[i-1] <- mean(simulations)
}

data <- data.frame(group_size = group_sizes, probability = probabilities)


ggplot(data, aes(x = group_size, y = probability)) +
  geom_line() + 
  geom_point() + 
  labs(
    x = "Group Size",
    y = "Probability of Shared Birthday",
    title = "Probability of At Least Two People Sharing a Birthday"
  ) +
  theme_minimal()
```

<img src="p8105_hw5_lmp2234_files/figure-gfm/unnamed-chunk-1-1.png" width="90%" />

The plot shows that as group size increases, the probability of two
people sharing the same birthday increases as well.

## Problem 2

### Setting the design elements

``` r
n <- 30
sigma <- 5
mu_values <-c(0,1,2,3,4,5,6)
n_sims <-5000 

sim_ttest <- function(mu = 0) {
  data <-  tibble(x = rnorm(n, mean = mu, sd = sigma))
  ttest <- t.test(x~1, data = data)
  tidy(ttest) %>%
    select(estimate, p.value) %>%
    mutate(mu = mu)
}
```

### Running a simulation of ttest for each value of mu and inserting into data frame. Creating a column indicating if null hypothesis is rejected if p-value is \<0.05. Calculating the statistical power for each mu.

``` r
results <- map(mu_values , function(mu) {
  replicate(n_sims, sim_ttest(mu), simplify = FALSE)
})

results <- lapply(results, as.data.frame)

results_df <- results %>%
  map_dfr(~ .x, .id = "mu") %>%
  mutate(mu = as.numeric(mu), 
         rejected = p.value < 0.05)

df_power <- results_df %>%
  group_by(mu) %>%
  summarize(power = mean(rejected,na.rm = TRUE), .groups = "drop" )
```

### Plot of the Proportion of Times Null was Rejected

``` r
ggplot(df_power, aes(x = mu, y = power))+
  geom_point () + 
  geom_line()+
  labs(x= "Mu", y = "Power")
```

<img src="p8105_hw5_lmp2234_files/figure-gfm/unnamed-chunk-4-1.png" width="90%" />

The association between effect size and power is that as the effect size
increases, the statistical power of the test increases as well. However,
the power cannot exceed 1 since the range goes from 0 to 1, as displayed
by the graph. As the effect size increases, the largest power it can
reach is 1.

### Part B - Second Plot

### Creating a table of the estimates, p-values, and mu to make for and binding into a single data frame for graphing.

``` r
estimate_cols <- grep("estimate", names(results_df), value = TRUE)
pvalue_cols <- grep("p.value", names(results_df), value = TRUE)

long_data <- list()

for (i in 1:length(estimate_cols)) {

   estimate_data <- results_df[[estimate_cols[i]]]
   pvalue_data <- results_df[[pvalue_cols[i]]]
  
   temp_data <- tibble(
     estimate = estimate_data,
     p.value = pvalue_data,
     mu = results_df[[paste0("mu.", i)]]  
   )
   
   long_data[[i]] <- temp_data
}

results_long <- bind_rows(long_data)
head(results_long)
```

    ## # A tibble: 6 √ó 3
    ##   estimate     p.value    mu
    ##      <dbl>       <dbl> <dbl>
    ## 1    0.319 0.795           0
    ## 2   -0.532 0.589           1
    ## 3    1.74  0.0617          2
    ## 4    3.39  0.000345        3
    ## 5    3.41  0.00119         4
    ## 6    5.16  0.000000114     5

### Creating a plot of the average estimates of ùúáÃÇ and the true ùúá. Creating a second plot of the average estimate of ùúáfor samples in which the null was rejected on y-axis and the true value of ùúá on the x-axis and overlaying.

``` r
ggplot(
  bind_rows(
    results_long %>%
      group_by(mu) %>%
      summarize(avg_estimate = mean(estimate, na.rm = TRUE), .groups = "drop") %>%
      mutate(type = "All Samples"),
    
    results_long %>%
      filter(p.value < 0.05) %>%
      group_by(mu) %>%
      summarize(avg_estimate = mean(estimate, na.rm = TRUE), .groups = "drop") %>%
      mutate(type = "Rejected Nulls")
  ),
  aes(x = mu, y = avg_estimate, color = type)
) +
 geom_point() + 
  geom_line() +
  labs(x = "True Mean (Mu)", y = "Average Estimate of Mu (MuÃÇ)", 
       title = "Average Estimate of Mu vs True Mu") +
  theme_minimal()
```

<img src="p8105_hw5_lmp2234_files/figure-gfm/unnamed-chunk-6-1.png" width="90%" />
The average of ùúáÃÇacross all tests for which the null is rejected is not
always approximately equal to the true value of ùúá. The distribution of
sample means may be skewed when you reject the null hypothesis since
rejecting the null means only including sample means different from the
null hypothesis (i.e., thus biasing).

## Problem 3

``` r
homocide_df = read.csv(
  "data/homicide-data.csv")

head(homocide_df)
```

    ##          uid reported_date victim_last victim_first victim_race victim_age
    ## 1 Alb-000001      20100504      GARCIA         JUAN    Hispanic         78
    ## 2 Alb-000002      20100216     MONTOYA      CAMERON    Hispanic         17
    ## 3 Alb-000003      20100601 SATTERFIELD      VIVIANA       White         15
    ## 4 Alb-000004      20100101    MENDIOLA       CARLOS    Hispanic         32
    ## 5 Alb-000005      20100102        MULA       VIVIAN       White         72
    ## 6 Alb-000006      20100126        BOOK    GERALDINE       White         91
    ##   victim_sex        city state      lat       lon           disposition
    ## 1       Male Albuquerque    NM 35.09579 -106.5386 Closed without arrest
    ## 2       Male Albuquerque    NM 35.05681 -106.7153      Closed by arrest
    ## 3     Female Albuquerque    NM 35.08609 -106.6956 Closed without arrest
    ## 4       Male Albuquerque    NM 35.07849 -106.5561      Closed by arrest
    ## 5     Female Albuquerque    NM 35.13036 -106.5810 Closed without arrest
    ## 6     Female Albuquerque    NM 35.15111 -106.5378        Open/No arrest

### Data Description

The dataset contains observations on 52179 criminal homocides over the
past decade in 50 of the largest American cities. The data include 12
variables, and are primarily demographic characteristics about the
victims, the location of the killings, and whether an arrest was made.

### Creating city_state variable and summarizing within cities

``` r
homocide_df <- homocide_df %>%
  mutate(city_state = paste(city, state, sep = ","))
  

summarize_homocide <- homocide_df %>%
  group_by(city_state)  %>%
  summarize(
    total_homocides = n(), 
    unsolved_homocides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")), 
    .groups = "drop"
  )

print(summarize_homocide)
```

    ## # A tibble: 51 √ó 3
    ##    city_state     total_homocides unsolved_homocides
    ##    <chr>                    <int>              <int>
    ##  1 Albuquerque,NM             378                146
    ##  2 Atlanta,GA                 973                373
    ##  3 Baltimore,MD              2827               1825
    ##  4 Baton Rouge,LA             424                196
    ##  5 Birmingham,AL              800                347
    ##  6 Boston,MA                  614                310
    ##  7 Buffalo,NY                 521                319
    ##  8 Charlotte,NC               687                206
    ##  9 Chicago,IL                5535               4073
    ## 10 Cincinnati,OH              694                309
    ## # ‚Ñπ 41 more rows

### Using prop test to estimate proportion of homocides that are unsolved in Baltimore, MD.

``` r
baltimore_df <- summarize_homocide  %>%
  filter(city_state == "Baltimore,MD")

prop_testdf <- prop.test(
  baltimore_df[["unsolved_homocides"]],
  baltimore_df[["total_homocides"]]
)

tidy_result <- tidy(prop_testdf)

estimate_proportion <- tidy_result [["estimate"]]
lower_ci <- tidy_result[["conf.low"]]
upper_ci <- tidy_result[["conf.high"]]

cat("Estimated Proportion of Unsolved Homocides", estimate_proportion, "\n")
```

    ## Estimated Proportion of Unsolved Homocides 0.6455607

``` r
cat("Confidence Interval: [", lower_ci, ",", upper_ci, "]\n")
```

    ## Confidence Interval: [ 0.6275625 , 0.6631599 ]

### Creating the table that lists estimates and confidence intervals per city

``` r
prop_df <- summarize_homocide  %>%
  mutate(prop_test = pmap(
    list(unsolved_homocides = summarize_homocide[["unsolved_homocides"]], 
         total_homocides = summarize_homocide[["total_homocides"]]), 
    ~ prop.test(..1, ..2)
  )) %>%
  mutate(tidy_results = map(prop_test, tidy)) %>%
  unnest(tidy_results) %>%
  select(city_state, estimate, conf.low, conf.high)
      
print(prop_df)
```

    ## # A tibble: 51 √ó 4
    ##    city_state     estimate conf.low conf.high
    ##    <chr>             <dbl>    <dbl>     <dbl>
    ##  1 Albuquerque,NM    0.386    0.337     0.438
    ##  2 Atlanta,GA        0.383    0.353     0.415
    ##  3 Baltimore,MD      0.646    0.628     0.663
    ##  4 Baton Rouge,LA    0.462    0.414     0.511
    ##  5 Birmingham,AL     0.434    0.399     0.469
    ##  6 Boston,MA         0.505    0.465     0.545
    ##  7 Buffalo,NY        0.612    0.569     0.654
    ##  8 Charlotte,NC      0.300    0.266     0.336
    ##  9 Chicago,IL        0.736    0.724     0.747
    ## 10 Cincinnati,OH     0.445    0.408     0.483
    ## # ‚Ñπ 41 more rows

### Creating a Plot of Estimates & CIs for Each City

### Organizing cities according to proportion of unsolved homocides and plotting

``` r
prop_df  %>%
  mutate(city_state = reorder(city_state, estimate))  %>%
  ggplot(aes(x=estimate, y=city_state))+
  geom_point(size = 3) +
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high), width = 0.1)+
  labs(
    x = "Proportion of Unsolved Homocides", 
    y = "City", 
    title = "Proportion of Unsolved Homocides by City with 95% CIs"
  )+
  theme_minimal()+
  theme(
    axis.text.y = element_text(size = 5),
  )
```

<img src="p8105_hw5_lmp2234_files/figure-gfm/unnamed-chunk-11-1.png" width="90%" />
